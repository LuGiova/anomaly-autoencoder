# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14juuLPnBzZEVSxucC5XtIhi8C4v_qMbh
"""

# === Librerie ===
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
import umap
from sklearn.preprocessing import MinMaxScaler
from sklearn.manifold import TSNE
from copy import deepcopy
import ipywidgets as widgets
from IPython.display import display
from sklearn.mixture import GaussianMixture
from sklearn.metrics import confusion_matrix
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.cluster import AgglomerativeClustering

# === Modello AE ===
class Encoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 2, 1)
        self.norm1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, 3, 2, 1)
        self.norm2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, 5, 2, 2)
        self.norm3 = nn.BatchNorm2d(128)
        self.fc = nn.Linear(128 * 13 * 13, 4)

    def forward(self, x):
        x = F.relu(self.norm1(self.conv1(x)))
        x = F.relu(self.norm2(self.conv2(x)))
        x = F.relu(self.norm3(self.conv3(x)))
        x = x.view(x.size(0), -1)
        return self.fc(x)

class Decoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(4, 64 * 25 * 25)
        self.deconv1 = nn.ConvTranspose2d(64, 32, 3, 2, 1, output_padding=1)
        self.dropout1 = nn.Dropout2d(0.1)
        self.deconv2 = nn.ConvTranspose2d(32, 16, 3, 2, 1, output_padding=1)
        self.dropout2 = nn.Dropout2d(0.1)
        self.deconv3 = nn.ConvTranspose2d(16, 1, 3, 1, 1)

    def forward(self, z):
        x = self.fc(z).view(-1, 64, 25, 25)
        x = F.relu(self.deconv1(x))
        x = self.dropout1(x)
        x = F.relu(self.deconv2(x))
        x = self.dropout2(x)
        x = torch.sigmoid(self.deconv3(x))
        return x

class AE(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()

    def forward(self, x):
        return self.decoder(self.encoder(x))

# === Funzioni utili ===
def ae_loss(recon_x, x, lambda_sim=0.005):
    bce = F.binary_cross_entropy(recon_x.view(recon_x.size(0), -1), x.view(x.size(0), -1), reduction='mean')
    recon_flat = recon_x.view(recon_x.size(0), -1)
    dot = recon_flat @ recon_flat.T
    sq_norms = (recon_flat ** 2).sum(1, keepdim=True)
    dists_sq = sq_norms + sq_norms.T - 2 * dot
    mask = ~torch.eye(recon_flat.size(0), dtype=bool, device=recon_x.device)
    penalty = torch.exp(-dists_sq[mask]).mean()
    return bce + lambda_sim * penalty

def calcola_anomaly_scores(model, dl, p=2):
    model.eval()
    z_list, score_list = [], []
    with torch.no_grad():
        for xb, _ in dl:
            xb = xb.to(next(model.parameters()).device)
            z = model.encoder(xb)
            z_flat = z.view(z.size(0), -1)
            norm = torch.norm(z_flat, p=p, dim=1)
            z_list.append(z_flat.cpu().numpy())
            score_list.append(norm.cpu().numpy())
    return np.concatenate(z_list), np.concatenate(score_list)

#   FUNZIONI PER L'ANALISI
def plot_latente_pca(z, scores, title):
    reducer = PCA(n_components=2)
    z_2d = reducer.fit_transform(z)
    colors = np.log1p(scores)

    plt.figure(figsize=(8, 6))
    sc = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=colors, cmap='inferno', s=5, alpha=0.7)

    cb = plt.colorbar(sc)
    cb.set_label('Anomaly Score (log1p)')
    ticks_real = np.linspace(scores.min(), scores.max(), 6)
    ticks_log = np.log1p(ticks_real)
    cb.set_ticks(ticks_log)
    cb.set_ticklabels([f"{v:.2e}" for v in ticks_real])

    plt.title(title)
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.tight_layout()
    plt.show()
def plot_latente_umap(z, scores, title):
    reducer = umap.UMAP(n_components=2, random_state=42)
    z_2d = reducer.fit_transform(z)
    colors = np.log1p(scores)
    plt.figure(figsize=(8,6))
    sc = plt.scatter(z_2d[:,0], z_2d[:,1], c=colors, cmap='inferno', s=5, alpha=0.7)
    cb = plt.colorbar(sc)
    cb.set_label('Anomaly Score (log1p)')
    ticks_real = np.linspace(scores.min(), scores.max(), 6)
    ticks_log = np.log1p(ticks_real)
    cb.set_ticks(ticks_log)
    cb.set_ticklabels([f"{v:.2e}" for v in ticks_real])
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    plt.title(title)
    plt.show()

def plot_latente_tsne(z, scores, title):
    reducer = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate='auto', init='random')
    z_2d = reducer.fit_transform(z)
    colors = np.log1p(scores)
    plt.figure(figsize=(8,6))
    sc = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=colors, cmap='inferno', s=5, alpha=0.7)
    cb = plt.colorbar(sc)
    cb.set_label('Anomaly Score (log1p)')
    ticks_real = np.linspace(scores.min(), scores.max(), 6)
    ticks_log = np.log1p(ticks_real)
    cb.set_ticks(ticks_log)
    cb.set_ticklabels([f"{v:.2e}" for v in ticks_real])
    plt.xlabel("t-SNE Dimension 1")
    plt.ylabel("t-SNE Dimension 2")
    plt.title(title)
    plt.tight_layout()
    plt.show()

def plot_histogram(scores_train, scores_test, thresholds, percentili_soglia, title, legend='Test'):
    plt.figure(figsize=(8, 4))
    plt.hist(scores_train, bins=60, alpha=0.6, label='Validation', color='green')
    plt.hist(scores_test, bins=60, alpha=0.6, label=legend, color='gray')
    cmap = cm.get_cmap('tab10', len(percentili_soglia))  # oppure 'plasma', 'Set1', ecc.
    for i, (soglia, perc) in enumerate(zip(thresholds, percentili_soglia)):
        color = cmap(i)
        percent_anom = 100 * (scores_test > soglia).mean()
        plt.axvline(soglia, linestyle='--', linewidth=2, color=color,
                    label=f'Soglia {perc}° perc ({percent_anom:.1f}% anom): AS={soglia:.4f}')
    plt.xlabel('Anomaly Score')
    plt.ylabel('Frequenza')
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.show()

def plot_loss_curves(train_losses, val_losses, epoch):
    plt.figure(figsize=(8,5))
    plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')
    plt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss')
    plt.vlines(epoch, 0, max(train_losses + val_losses), colors='red', linestyles='dashed', label='Epoca ottimale')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.yscale('log')
    plt.title('Andamento della Loss durante il training')
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_anomaly_trend_by_percentile(anomalie_testl, anomalie_testh, percentile_soglia, epochs, epoch):
    for i, perc in enumerate(percentile_soglia):
        plt.figure(figsize=(10, 5))
        y_low = np.array(anomalie_testl[i]) / 3000
        y_high = np.array(anomalie_testh[i]) / 3000
        plt.plot(range(1, epochs + 1), y_low, label='Test Low')
        plt.plot(range(1, epochs + 1), y_high, label='Test High')
        plt.hlines([0.45, 0.55], xmin=0, xmax=epochs, colors='gray', linestyles='dashed', label='Soglie 0.45 / 0.55')
        plt.vlines(epoch, 0, 1, colors='red', linestyles='dashed', label='Epoca ottimale')
        plt.xlabel('Epoca')
        plt.ylabel('Percentuale anomalie rilevate')
        plt.title(f'Evoluzione anomalie rilevate – {perc}° percentile')
        plt.ylim(0, 1)
        plt.legend()
        plt.grid(True)
        plt.show()

def mostra_ricostruzioni_slider(originali, ricostruzioni_epoch, cmap='hot', spazio=2, vmin=0, vmax=1, title_fontsize=16):
    slider = widgets.IntSlider(min=1, max=len(ricostruzioni_epoch), step=1, description='Epoca')
    out = widgets.Output()

    def concatena_striscia_con_spazi(batch):
        batch = batch[:, 0]  # shape: (N, 100, 100)
        separatore = np.ones((batch[0].shape[0], spazio))  # bordo bianco (valore 1)
        striscia = np.hstack([np.hstack([img, separatore]) for img in batch])[:, :-spazio]
        return striscia

    def update(change):
        epoca = change['new'] - 1
        recon_batch = ricostruzioni_epoch[epoca]
        with out:
            out.clear_output(wait=True)
            fig, axs = plt.subplots(2, 1, figsize=(36, 8))
            axs[0].imshow(concatena_striscia_con_spazi(originali), cmap=cmap, vmin=vmin, vmax=vmax)
            axs[0].set_title("Input originale", fontsize=title_fontsize)
            axs[0].axis('off')
            axs[1].imshow(concatena_striscia_con_spazi(recon_batch), cmap=cmap, vmin=vmin, vmax=vmax)
            axs[1].set_title(f"Ricostruzione - Epoca {epoca+1}", fontsize=title_fontsize)
            axs[1].axis('off')
            plt.tight_layout()
            plt.show()

    slider.observe(update, names='value')
    display(slider, out)
    update({'new': 1})  # Mostra epoca 1 all'inizio


def purity_score(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    return np.sum(np.amax(cm, axis=0)) / np.sum(cm)

def plot_latent_embedding_comparativo(z, score, labels, threshold, title_prefix='', vmin=None, vmax=None):
    reducers = {
        'PCA': PCA(n_components=2),
        'UMAP': umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42),
        't-SNE': TSNE(n_components=2, perplexity=30, random_state=42)
    }

    c = np.log1p(score)
    dimension = 25
    markers = ['.', 'v']

    if vmin is None:
        vmin = c.min()
    if vmax is None:
        vmax = c.max()

    # Determina quale cluster ha la maggioranza di anomalie
    cluster_ids = np.unique(labels)
    cluster_to_label = {}
    for cl in cluster_ids:
        mask = labels == cl
        is_anomaly = (score[mask] > threshold)
        majority_anomaly = is_anomaly.mean() > 0.5
        cluster_to_label[cl] = 'Anomalo' if majority_anomaly else 'Normale'

    # Crea subplot 1x3
    fig, axs = plt.subplots(1, 3, figsize=(21, 6))

    for ax, (name, reducer) in zip(axs, reducers.items()):
        emb = reducer.fit_transform(z)
        for i, cl in enumerate(cluster_ids):
            idx = labels == cl
            sc = ax.scatter(
                emb[idx, 0], emb[idx, 1],
                c=c[idx],
                cmap='inferno',
                marker=markers[i % len(markers)],
                edgecolors='none',
                s=dimension,
                alpha=0.7,
                vmin=vmin,
                vmax=vmax,
                label=f"Cluster {cl} ({cluster_to_label[cl]})"
            )
        ax.set_title(f"{title_prefix} - {name}")
        ax.legend()

    # Colorbar unica
    cbar_ax = fig.add_axes([0.93, 0.15, 0.015, 0.7])
    cb = fig.colorbar(sc, cax=cbar_ax)
    cb.set_label('Anomaly Score (log1p)')
    ticks_real = np.linspace(score.min(), score.max(), 6)
    ticks_log = np.log1p(ticks_real)
    cb.set_ticks(ticks_log)
    cb.set_ticklabels([f"{v:.2e}" for v in ticks_real])

    plt.tight_layout(rect=[0, 0, 0.91, 1])
    plt.show()
